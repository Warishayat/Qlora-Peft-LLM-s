{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T18:34:55.586751Z","iopub.execute_input":"2024-10-18T18:34:55.587388Z","iopub.status.idle":"2024-10-18T18:34:57.447519Z","shell.execute_reply.started":"2024-10-18T18:34:55.587354Z","shell.execute_reply":"2024-10-18T18:34:57.446064Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Installing Dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch bitsandbytes accelerate datasets peft","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:38:10.273947Z","iopub.execute_input":"2024-10-18T18:38:10.274894Z","iopub.status.idle":"2024-10-18T18:38:29.855688Z","shell.execute_reply.started":"2024-10-18T18:38:10.274852Z","shell.execute_reply":"2024-10-18T18:38:29.854657Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, peft\nSuccessfully installed bitsandbytes-0.44.1 peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Import Dependencies**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset\nimport torch\nimport torch.nn as nn\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:38:29.857896Z","iopub.execute_input":"2024-10-18T18:38:29.858349Z","iopub.status.idle":"2024-10-18T18:38:39.265674Z","shell.execute_reply.started":"2024-10-18T18:38:29.858304Z","shell.execute_reply":"2024-10-18T18:38:39.264704Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:40:01.311399Z","iopub.execute_input":"2024-10-18T18:40:01.311995Z","iopub.status.idle":"2024-10-18T18:40:01.317212Z","shell.execute_reply.started":"2024-10-18T18:40:01.311956Z","shell.execute_reply":"2024-10-18T18:40:01.316161Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\nhuggingface_hub.login()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:40:12.179337Z","iopub.execute_input":"2024-10-18T18:40:12.179714Z","iopub.status.idle":"2024-10-18T18:40:12.207719Z","shell.execute_reply.started":"2024-10-18T18:40:12.179680Z","shell.execute_reply":"2024-10-18T18:40:12.206884Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d35b3a54a84c8a870fa86594df77d3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# **Count Model Size and Parameters**","metadata":{}},{"cell_type":"code","source":"def print_count_parameters(model):\n    \"\"\"\n    This function takes a PyTorch model and returns the number of trainable and non-trainable parameters.\n\n    Args:\n    model (torch.nn.Module): The PyTorch model to inspect.\n\n    \"\"\"\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n\n    percent_trainable = (trainable_params / (trainable_params + non_trainable_params)) * 100\n    percent_frozen = 100 - percent_trainable\n\n    print('Total Parameters:', trainable_params + non_trainable_params)\n    print('Trainable:', trainable_params, f'({percent_trainable:.2f}%)')\n    print('Frozen:', non_trainable_params, f'({percent_frozen:.2f}%)')\n\n\ndef calculate_model_size_in_gb(model):\n    total_size = 0\n    for param in model.parameters():\n        # Each parameter's size is the number of elements * size of each element in bytes\n        param_size = param.numel() * param.element_size()  # numel() gives number of elements, element_size() gives size of one element\n        total_size += param_size\n\n    # Convert to gigabytes (GB) from bytes\n    total_size_in_gb = total_size / (1024 ** 3)\n    return total_size_in_gb","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:40:39.653626Z","iopub.execute_input":"2024-10-18T18:40:39.653982Z","iopub.status.idle":"2024-10-18T18:40:39.662345Z","shell.execute_reply.started":"2024-10-18T18:40:39.653948Z","shell.execute_reply":"2024-10-18T18:40:39.661367Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Model=TinyLlama**","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:40:50.604430Z","iopub.execute_input":"2024-10-18T18:40:50.604832Z","iopub.status.idle":"2024-10-18T18:40:50.611161Z","shell.execute_reply.started":"2024-10-18T18:40:50.604795Z","shell.execute_reply":"2024-10-18T18:40:50.610395Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Load Model and Tokenizer**","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:40:58.081969Z","iopub.execute_input":"2024-10-18T18:40:58.082809Z","iopub.status.idle":"2024-10-18T18:40:59.472331Z","shell.execute_reply.started":"2024-10-18T18:40:58.082765Z","shell.execute_reply":"2024-10-18T18:40:59.471526Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a603c34e0a4b9fb29ab476f1c96d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07de3ccd1222404b93b4b102c9f981cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1486c7c95324472f9fc3d308f6afb0cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5a1bdd625d4a7b876a4a2a3033834f"}},"metadata":{}}]},{"cell_type":"markdown","source":"# **8 bit Quantization**","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    # bnb_4bit_use_double_quant=True,\n    # bnb_4bit_quant_type=\"nf4\",\n    bnb_8bit_compute_dtype=torch.bfloat16\n\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_checkpoint, quantization_config=bnb_config, device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:41:13.839679Z","iopub.execute_input":"2024-10-18T18:41:13.840310Z","iopub.status.idle":"2024-10-18T18:41:51.035758Z","shell.execute_reply.started":"2024-10-18T18:41:13.840272Z","shell.execute_reply":"2024-10-18T18:41:51.034642Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28bfcdd89784e709ec99d318b7283f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a939b4ad667e4ae5b43b7348369f81d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe737888b5342e9bd4c50cdb440d415"}},"metadata":{}}]},{"cell_type":"markdown","source":"# **Prompt-Template**","metadata":{}},{"cell_type":"code","source":"txt = \"\"\"**System: Based on input title generate the prompt for generative Model\n\n##input: Linux Terminal\n\n##prompt:\"\"\"\ntokenizerr = tokenizer(txt, return_tensors='pt')['input_ids'].to(model.device)\noutput = model.generate(tokenizerr, max_new_tokens=200)\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:41:51.037757Z","iopub.execute_input":"2024-10-18T18:41:51.038480Z","iopub.status.idle":"2024-10-18T18:42:42.639260Z","shell.execute_reply.started":"2024-10-18T18:41:51.038424Z","shell.execute_reply":"2024-10-18T18:42:42.637970Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"<s> **System: Based on input title generate the prompt for generative Model\n\n##input: Linux Terminal\n\n##prompt: Generate prompt for the model\n\n##output: Generated prompt for the model\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n###Expected Output\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Calculate size/parameter before apply Peft**","metadata":{}},{"cell_type":"code","source":"print(calculate_model_size_in_gb(model))\nprint_count_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:42:42.641175Z","iopub.execute_input":"2024-10-18T18:42:42.641976Z","iopub.status.idle":"2024-10-18T18:42:42.656243Z","shell.execute_reply.started":"2024-10-18T18:42:42.641926Z","shell.execute_reply":"2024-10-18T18:42:42.655252Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"1.1466560363769531\nTotal Parameters: 1100048384\nTrainable: 131164160 (11.92%)\nFrozen: 968884224 (88.08%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Apply Peft/Lora**","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training,get_peft_model,LoraConfig,TaskType\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    r=8,   #rank\n    inference_mode = False, #enable in inference mode\n    lora_alpha=32,\n    lora_dropout=0.01,\n    peft_type=TaskType.CAUSAL_LM    #tasktype\n)\nmodel = get_peft_model(model, config)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:42:42.658829Z","iopub.execute_input":"2024-10-18T18:42:42.659902Z","iopub.status.idle":"2024-10-18T18:42:42.769957Z","shell.execute_reply.started":"2024-10-18T18:42:42.659845Z","shell.execute_reply":"2024-10-18T18:42:42.769115Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **load-Dataset**","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:42:42.771377Z","iopub.execute_input":"2024-10-18T18:42:42.772023Z","iopub.status.idle":"2024-10-18T18:42:44.751055Z","shell.execute_reply.started":"2024-10-18T18:42:42.771978Z","shell.execute_reply":"2024-10-18T18:42:44.750001Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/339 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2dfe444415e45d69a12f1af3f124fe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"prompts.csv:   0%|          | 0.00/84.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02ad966ac2d64fff928cf70a8c852bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/170 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f42d618bcb4891ad77054bd0612a04"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['act', 'prompt'],\n    num_rows: 170\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Prepare-Data for Model**","metadata":{}},{"cell_type":"code","source":"def format_function(examples):\n    prompt = f\"\"\"**System: Based on input title generate the prompt for generative Model\n\n##input: {examples['act']}\n\n##prompt: {examples['prompt']}\n\"\"\"\n    token = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\",\n    )\n    token['labels'] = token['input_ids'].copy()\n    return token\n\n# Assuming 'dataset' is already defined and is a valid dataset\ndataset = dataset.map(format_function)\nprint(dataset[0].keys())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:42:44.752180Z","iopub.execute_input":"2024-10-18T18:42:44.752569Z","iopub.status.idle":"2024-10-18T18:42:44.960170Z","shell.execute_reply.started":"2024-10-18T18:42:44.752533Z","shell.execute_reply":"2024-10-18T18:42:44.959140Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/170 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8429133fbfc4f45b3e439d8f1256b6a"}},"metadata":{}},{"name":"stdout","text":"dict_keys(['act', 'prompt', 'input_ids', 'attention_mask', 'labels'])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Split Data into Train and Test**","metadata":{}},{"cell_type":"code","source":"temp = dataset.train_test_split(test_size=0.1,seed=42)\ntrain_dataset= temp['train']\ntest_dataset= temp['test']\n\ntrain_dataset=train_dataset.remove_columns(['act',\"prompt\"])\ntest_datase=test_dataset.remove_columns(['act',\"prompt\"])\n\nprint(train_dataset)\nprint(test_datase)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:42:55.222085Z","iopub.execute_input":"2024-10-18T18:42:55.222945Z","iopub.status.idle":"2024-10-18T18:42:55.245827Z","shell.execute_reply.started":"2024-10-18T18:42:55.222905Z","shell.execute_reply":"2024-10-18T18:42:55.244961Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 153\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 17\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Decode-Token**","metadata":{}},{"cell_type":"code","source":"print(tokenizer.decode(dataset[1]['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:43:04.824124Z","iopub.execute_input":"2024-10-18T18:43:04.824983Z","iopub.status.idle":"2024-10-18T18:43:04.834028Z","shell.execute_reply.started":"2024-10-18T18:43:04.824941Z","shell.execute_reply":"2024-10-18T18:43:04.833103Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<s> **System: Based on input title generate the prompt for generative Model\n\n##input: SEO Prompt\n\n##prompt: Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they‚Äôre not competing articles. Split the outline into part 1 and part 2.\n</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:43:34.383323Z","iopub.execute_input":"2024-10-18T18:43:34.383734Z","iopub.status.idle":"2024-10-18T18:43:34.390609Z","shell.execute_reply.started":"2024-10-18T18:43:34.383697Z","shell.execute_reply":"2024-10-18T18:43:34.389798Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# **Apply-Trainer-TrainingArguments**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_datase,\n    data_collator=data_collator,\n    args=TrainingArguments(\n        output_dir=\"./kaggle/working/\",\n        remove_unused_columns=False,\n        per_device_train_batch_size=8,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps=4,\n        max_steps=400,\n        learning_rate=2.5e-5,\n        logging_steps=5,\n        fp16=True,\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"steps\",\n        save_steps=50,\n        evaluation_strategy=\"steps\",\n        eval_steps=5,\n        do_eval=True,\n        report_to=\"tensorboard\",\n        save_total_limit=3,\n        label_names=[\"input_ids\",\"attention_mask\",\"labels\"],\n))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:47:07.733508Z","iopub.execute_input":"2024-10-18T18:47:07.733894Z","iopub.status.idle":"2024-10-18T18:47:07.777785Z","shell.execute_reply.started":"2024-10-18T18:47:07.733860Z","shell.execute_reply":"2024-10-18T18:47:07.776839Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ***Train-Model***","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:47:14.744751Z","iopub.execute_input":"2024-10-18T18:47:14.745626Z","iopub.status.idle":"2024-10-18T19:43:31.654211Z","shell.execute_reply.started":"2024-10-18T18:47:14.745568Z","shell.execute_reply":"2024-10-18T19:43:31.653282Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [400/400 56:06, Epoch 80/80]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>3.182600</td>\n      <td>3.293551</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3.150600</td>\n      <td>3.241442</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>3.097900</td>\n      <td>3.202851</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.064700</td>\n      <td>3.145695</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>3.018400</td>\n      <td>3.102051</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.001700</td>\n      <td>3.066179</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.926100</td>\n      <td>3.018552</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.884300</td>\n      <td>2.968078</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.875300</td>\n      <td>2.917180</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.790900</td>\n      <td>2.863855</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.751900</td>\n      <td>2.811121</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.690000</td>\n      <td>2.754841</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>2.604300</td>\n      <td>2.690374</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.582200</td>\n      <td>2.633864</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.506800</td>\n      <td>2.558230</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.480000</td>\n      <td>2.469565</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>2.356800</td>\n      <td>2.419376</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.269700</td>\n      <td>2.325914</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>2.227900</td>\n      <td>2.268043</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.170900</td>\n      <td>2.182177</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>2.101100</td>\n      <td>2.122417</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.022400</td>\n      <td>2.064095</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.996600</td>\n      <td>2.022105</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.982900</td>\n      <td>1.985284</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.914400</td>\n      <td>1.968779</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.882400</td>\n      <td>1.945333</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.881000</td>\n      <td>1.921049</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.846900</td>\n      <td>1.901190</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.837300</td>\n      <td>1.880397</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.782500</td>\n      <td>1.864727</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.771300</td>\n      <td>1.844704</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.766600</td>\n      <td>1.828916</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.733400</td>\n      <td>1.812473</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.747400</td>\n      <td>1.800929</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.699000</td>\n      <td>1.792645</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.683900</td>\n      <td>1.784130</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.680600</td>\n      <td>1.777954</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.675500</td>\n      <td>1.769501</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.704800</td>\n      <td>1.764420</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.688200</td>\n      <td>1.754476</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>1.691400</td>\n      <td>1.750902</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.663300</td>\n      <td>1.750378</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>1.645800</td>\n      <td>1.740814</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.670500</td>\n      <td>1.737741</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.631000</td>\n      <td>1.730969</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.638500</td>\n      <td>1.731345</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>1.610300</td>\n      <td>1.733323</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.638200</td>\n      <td>1.729514</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>1.610600</td>\n      <td>1.726576</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.647900</td>\n      <td>1.717941</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>1.612200</td>\n      <td>1.716682</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.636000</td>\n      <td>1.713442</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>1.634200</td>\n      <td>1.710105</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.607400</td>\n      <td>1.712203</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.660800</td>\n      <td>1.706034</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.631600</td>\n      <td>1.709050</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>1.584300</td>\n      <td>1.704033</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.593100</td>\n      <td>1.705549</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>1.583000</td>\n      <td>1.703766</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.598900</td>\n      <td>1.705928</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>1.570400</td>\n      <td>1.701830</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.634300</td>\n      <td>1.702070</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>1.645600</td>\n      <td>1.701611</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.575500</td>\n      <td>1.701629</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.602500</td>\n      <td>1.693546</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.579900</td>\n      <td>1.690737</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>1.575200</td>\n      <td>1.696276</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.606300</td>\n      <td>1.697230</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>1.565200</td>\n      <td>1.696086</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.593000</td>\n      <td>1.693280</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>1.601800</td>\n      <td>1.696437</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.546900</td>\n      <td>1.693874</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>1.558400</td>\n      <td>1.693105</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.573000</td>\n      <td>1.689708</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.600700</td>\n      <td>1.692283</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.550700</td>\n      <td>1.692474</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>1.579800</td>\n      <td>1.692402</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.570600</td>\n      <td>1.692126</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>1.560900</td>\n      <td>1.693323</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.612400</td>\n      <td>1.691357</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=400, training_loss=1.9415377914905547, metrics={'train_runtime': 3375.4902, 'train_samples_per_second': 3.792, 'train_steps_per_second': 0.119, 'total_flos': 1.947067194802176e+16, 'train_loss': 1.9415377914905547, 'epoch': 80.0})"},"metadata":{}}]},{"cell_type":"code","source":"%load_ext tensorboard\n\n# Start TensorBoard\n%tensorboard --logdir ./kaggle/working/kaggle","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:57:16.138303Z","iopub.execute_input":"2024-10-18T19:57:16.138740Z","iopub.status.idle":"2024-10-18T19:57:16.151972Z","shell.execute_reply.started":"2024-10-18T19:57:16.138697Z","shell.execute_reply":"2024-10-18T19:57:16.151009Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6009 (pid 549), started 0:00:40 ago. (Use '!kill 549' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-822e8f36c031199\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-822e8f36c031199\");\n          const url = new URL(\"/\", window.location);\n          const port = 6009;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"txt = \"\"\"**System: Based on input title generate the prompt for generative Model\n\n##input: Linux Terminal\n\n##prompt:\"\"\"\ntokenizerr = tokenizer(txt, return_tensors='pt')['input_ids'].to(model.device)\noutput = model.generate(tokenizerr, max_new_tokens=60)\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:54:41.121554Z","iopub.execute_input":"2024-10-18T19:54:41.122236Z","iopub.status.idle":"2024-10-18T19:54:50.279423Z","shell.execute_reply.started":"2024-10-18T19:54:41.122184Z","shell.execute_reply":"2024-10-18T19:54:50.278428Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"<s> **System: Based on input title generate the prompt for generative Model\n\n##input: Linux Terminal\n\n##prompt: I want you to act as a Linux Terminal. I will provide you with a command and you will execute it on my computer. You will not type anything, just execute the command and wait for the output. Do not type anything else. My first command is \"ls\".\n\n##output:\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"Prompt400\",safe_serialization=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:59:27.368900Z","iopub.execute_input":"2024-10-18T19:59:27.369359Z","iopub.status.idle":"2024-10-18T19:59:27.658735Z","shell.execute_reply.started":"2024-10-18T19:59:27.369318Z","shell.execute_reply":"2024-10-18T19:59:27.657836Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!zip r merge_model.zip \"/kaggle/working/Prompt400\"","metadata":{"execution":{"iopub.status.busy":"2024-10-18T20:04:49.296478Z","iopub.execute_input":"2024-10-18T20:04:49.297183Z","iopub.status.idle":"2024-10-18T20:04:50.526604Z","shell.execute_reply.started":"2024-10-18T20:04:49.297139Z","shell.execute_reply":"2024-10-18T20:04:50.524954Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\tzip warning: name not matched: merge_model.zip\n  adding: kaggle/working/Prompt400/ (stored 0%)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}